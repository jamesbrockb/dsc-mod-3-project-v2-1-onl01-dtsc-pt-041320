{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: James Brochhausen\n",
    "* Student pace: part time\n",
    "* Scheduled project review date/time: 10/08/2020\n",
    "* Instructor name: James Irving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following notebook we will be uncovering information from a dataset. Particularly, we will be understanding why a customer churns. To do this we will be walking through two classification models. The data we will be reviewing will be for a telecommunication. What we explore throughout this is, which features (columns) are the most important. We also need to understand their importance, meaning, is it important because it causes churn or is it important because it does not cause churn. This will be the main goal to understand throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#importing necessary functions.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn import tree\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import plot_confusion_matrix, recall_score\n",
    "from xgboost import XGBRFClassifier,XGBClassifier\n",
    "import shap\n",
    "from imblearn.over_sampling import SMOTE\n",
    "shap.initjs()\n",
    "import warnings\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Function\n",
    "def classify(y_true, y_pred,X_true,clf,cm_kws=dict(cmap=\"Greens\",\n",
    "                                  normalize='true'),figsize=(9,5),\n",
    "                   plot_roc_auc=True):\n",
    "    \n",
    "    # Class report \n",
    "    print(metrics.classification_report(y_true,y_pred))\n",
    "\n",
    "    if plot_roc_auc:\n",
    "        num_cols=1\n",
    "    else:\n",
    "        num_cols=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Opening my data\n",
    "df = pd.read_csv('bigml_59c28831336c6604c800002a.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.plotting.scatter_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_f = df['churn'].value_counts()\n",
    "ax = sns.barplot(t_f.index, t_f.values).set_title('Total Churn Vs. No Churn')\n",
    "# ax.set(xlabel = 'Churn', ylabel = 'Total Amount')\n",
    "# ax.set(xlabel='Churn', ylabel='Client Volume')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Client Volume')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=len(columns), ncols=1, figsize=(14,4))\n",
    "\n",
    "days = ['account length']\n",
    "minutes = ['total day minutes','total eve minutes','total night minutes',\n",
    "           'total intl minutes', 'customer service calls']\n",
    "calls = ['total day calls','total eve calls','total night calls',\n",
    "         'total intl calls']\n",
    "charge = ['total day charge','total eve charge','total night charge',\n",
    "          'total intl charge']\n",
    "\n",
    "mean = df.groupby('churn').mean()\n",
    "print(mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20,10))\n",
    "for i, col in enumerate(minutes):  \n",
    "    sns.catplot(x='churn', y=col, data=df, kind = 'swarm', ax=ax[i//3][i%3])\n",
    "    plt.close()\n",
    "#Take a look at violin plot and Swarm.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(20,10))\n",
    "for i, col in enumerate(calls):  \n",
    "    sns.catplot(x='churn', y=col, data=df, kind = 'swarm', split = True,\n",
    "                hue = 'churn', ax=ax[i//2][i%2])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(20,10))\n",
    "for i, col in enumerate(charge):  \n",
    "    sns.catplot(x='churn', y=col, data=df, kind = 'swarm', ax=ax[i//2][i%2])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Phone Number\n",
    "# Setting my X and y data\n",
    "y = df['churn'].astype(int)\n",
    "X = df.drop(columns = ['churn', 'phone number']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_sm, y_sm = SMOTE().fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale = StandardScaler()\n",
    "\n",
    "X_train_scaled = std_scale.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns = X.columns)\n",
    "X_test_scaled = std_scale.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = SMOTE().fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resampled dataset shape %s' % Counter(y_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=10)\n",
    "\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test,y_pred) * 100\n",
    "print(('Accuracy :{0}'.format(acc))),2\n",
    "\n",
    "# AUC\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,\n",
    "                                                                y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('\\nAUC :{0}'.format(round(roc_auc, 2)))\n",
    "\n",
    "# Printed confusion matrix \n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'],\n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classify(y_test,y_pred,X_test_scaled,rf_clf)\n",
    "\n",
    "plot_confusion_matrix(rf_clf, X_test_scaled, y_test, values_format='.3g',\n",
    "                     normalize = 'true')\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_rf = XGBClassifier()\n",
    "xgb_rf.fit(X_train, y_train)\n",
    "# print('Training score: ' ,round(xgb_rf.score(X_train,y_train),2))\n",
    "# print('Test score: ',round(xgb_rf.score(X_test,y_test),2))\n",
    "\n",
    "y_pred2 = xgb_rf.predict(X_test)\n",
    "\n",
    "classify(y_test,y_pred2,X_test,xgb_rf)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(xgb_rf, X_test, y_pred2, values_format='.3g', \n",
    "                      normalize = 'true')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xgb_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_train_scaled,y_train)\n",
    "shap.summary_plot(shap_values, X_train_scaled, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values,X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_score = cross_val_score(rf_clf, X_train, y_train, cv=3)\n",
    "\n",
    "mean_best_score = np.mean(best_score)\n",
    "print(mean_best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 2, 4, 5, 6, 8, 10, 12],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [2,4, 6, 8, 10, 12, 14],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_grid_search = GridSearchCV(rf_clf, grid_params, scoring = 'recall', cv=3,\n",
    "                              return_train_score=True)\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_acc = rf_grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Recall Score : ' + str(recall_score(y_test,y_pred_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rf_gs_training_score = np.mean(rf_grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# # Mean test score\n",
    "# rf_gs_testing_score = rf_grid_search.score(X_test, y_test)\n",
    "\n",
    "# print(f\"Avg. Training Score: {rf_gs_training_score :.2%}\")\n",
    "# print(f\"Avg. Test Score: {rf_gs_testing_score :.2%}\")\n",
    "# print('Recall Score : ' + str(recall_score(y_test,y_pred2)))\n",
    "# # print(\"Best Param Combo:\")\n",
    "# # rf_grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(xgb_rf, X_test, y_pred_acc, values_format='.3g', \n",
    "                      normalize = 'true')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_score = rf_grid_search.score(X_test, y_test)\n",
    "\n",
    "print(rf_score*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the exploration of the dataset we can conclude on a few things. That our model performed very well. We found a 60% accuracy using our random forests model, a 95% accuracy after apply XGB Boost and finally a 93% accuracy with our gridsearch parameters. The other important result we wanted to look at was Recall. After reviewing the information we can confirm that the top 5 most important features are:\n",
    "\n",
    "- International Plan Number (less likely to churn)\n",
    "- Total International Calls (less likely to churn)\n",
    "- Total International Minutes (less likely to churn)\n",
    "- Customer Service Calls (more likely to churn)\n",
    "- Voicemail Plan (less likely to churn)\n",
    "\n",
    "Moving forward, we recommend that our telecommunications client looks into the increasing the payment plan that goes into their international plan. We found that those who do have it are less likely to churn, so there may be an opportunity to make more money. Those who do have an international plan and have higher minutes and calls are less likely to churn. But those who do not use it as frequently as the others have been found to churn more frequently. We'd recommend looking at different pricing options for lower minute / call volumes from users. I'd recommend reducing the amount of customer service calls. The customers problems should be solved by the first or second call. The more calls they make the higher the likelihood there is of this customer churning. Finally with the voicemail plan we'd recommend looking into the pricing plan as well. There is another opportunity here to lift prices and potentially make more money.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "297.326px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
